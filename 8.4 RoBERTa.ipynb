{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta 分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载文件及模型\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(\"out.log\")\n",
    "import numpy\n",
    "import torch.cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载模型\n",
    "model_path = \"/data/liyunhan/Model/chinese-roberta-wwm-ext\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path,num_labels=2)\n",
    "## 注意num_labels\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义数据集, 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义与加载数据集\n",
    "class DatasetClassify(Dataset):\n",
    "    def __init__(self, path):\n",
    "        # 使用pandas读取CSV文件\n",
    "        df = pd.read_csv(path,encoding='utf-8')\n",
    "        self.data_list = df.to_dict('records')  # 将DataFrame转换成list of dictionaries\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data_list[index]\n",
    "        content = item['content']\n",
    "        label = item['label']\n",
    "\n",
    "        # 如果需要特殊的分隔符，例如'[SEP]', 可以在这里添加\n",
    "        # 但通常对于文本分类任务，我们不需要在输入文本中加入分隔符\n",
    "        # content = content + '[SEP]'  # 只有当需要时才使用\n",
    "\n",
    "        return content, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "def collator_fn(batch):\n",
    "    batch = numpy.array(batch)\n",
    "\n",
    "    data_batch = batch[:, 0]\n",
    "    label_batch = numpy.array(batch[:, 1], dtype=int)\n",
    "    data_batch = tokenizer(data_batch.tolist(), max_length=256, padding=True, truncation=True,\n",
    "                           return_tensors=\"pt\").to(DEVICE)\n",
    "    return data_batch, torch.tensor(label_batch, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "train_data_loader = DataLoader(DatasetClassify(\"/data/liyunhan/Model/data3.csv\"), batch_size=32, shuffle=True,\n",
    "                               collate_fn=collator_fn)\n",
    "dev_data_loader = DataLoader(DatasetClassify(\"/data/liyunhan/Model/data3.csv\"), batch_size=32, shuffle=False,\n",
    "                             collate_fn=collator_fn)\n",
    "\n",
    "## 定义测试函数\n",
    "@torch.no_grad()\n",
    "def eval():\n",
    "    num_true = 0\n",
    "    num_total = 0\n",
    "    for item, label in tqdm(dev_data_loader, position=0, leave=True):\n",
    "        output = model(**item, labels=label)\n",
    "        pre_label = output.logits.detach().cpu().numpy()\n",
    "        real_label = label.detach().cpu().numpy()\n",
    "        pre_label = np.argmax(pre_label, axis=1)\n",
    "        num_true += np.sum(real_label == pre_label)\n",
    "        num_total += len(pre_label)\n",
    "    acc = num_true/num_total\n",
    "    logger.info(\"\\n\" + str(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 开始训练\n",
    "EPOCHS = 46\n",
    "step = 0\n",
    "accu_max = 0.0\n",
    "loss_total_min = 20\n",
    "num_training_steps = len(train_data_loader) * EPOCHS\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_total = 0.0\n",
    "    for index, (item, label) in enumerate(tqdm(train_data_loader), start=1):\n",
    "        step = epoch * len(train_data_loader) + index\n",
    "        output = model(labels=label, **item)\n",
    "        loss = output.loss\n",
    "        loss_total += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        logger.info(f\"第{epoch}轮的损失为{loss_total}\")\n",
    "        \n",
    "        model.eval()\n",
    "        accu_score = eval()\n",
    "        model.train()\n",
    "        if accu_score > accu_max or loss_total<loss_total_min:\n",
    "            accu_max = accu_score\n",
    "            loss_total_min = loss_total\n",
    "            torch.save(model, \"/data/liyunhan/Model/Model_saved/classify_model.pt\")\n",
    "            print(\"保存模型\")\n",
    "        if epoch > 0:\n",
    "            LR = LR * 0.6\n",
    "        loss_total = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "model_path = \"/data/liyunhan/Model/chinese-roberta-wwm-ext\"\n",
    "classify_model = torch.load(\"/data/liyunhan/Model/Model_saved/classify_model.pt\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classify_model.to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def predict_single_sentence(sentence: str):\n",
    "    # Tokenize the sentence with the same settings as during training\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=256, padding=True, truncation=True)\n",
    "    # Move the input tensors to the correct device\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    # Set the model to evaluation mode (important if your model has layers like dropout or batchnorm)\n",
    "    classify_model.eval()\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = classify_model(**inputs)\n",
    "    #print(outputs)\n",
    "    \n",
    "    # Get the predicted class. This assumes that you're using a classification model\n",
    "    # and that the model returns logits.\n",
    "    # You might need to modify this depending on what your model's forward pass returns\n",
    "    _, predicted = torch.max(outputs.logits, 1)\n",
    "    \n",
    "    return predicted.item()  # Convert the tensor to a Python scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用测试集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data/liyunhan/Model/data3.csv\",encoding = 'utf-8')\n",
    "\n",
    "n=0\n",
    "id_list = []\n",
    "for i in range(len(df['content'])):\n",
    "    label = df['label'][i]\n",
    "    id = predict_single_sentence(df['content'][i])\n",
    "    id_list.append(id)\n",
    "    if label == id:\n",
    "        n = n+1\n",
    "print(\"准确率为:\",n/len(df['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手动输入内容测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\\n4. 条款：原文中提到“双方均有权向甲方所在地人民法院起诉。”\\n判断过程：原文明确约定了甲方所在地法院作为管辖机构，没有提及乙方所在地法院或仲裁机构。\\n结论：存在该问题。'\"\n",
    "id = predict_single_sentence(content)\n",
    "print(id)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
